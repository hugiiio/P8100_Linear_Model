---
title: "Linear regression"
output: html_document
---
```{r}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(mgcv)

set.seed(1)

```

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)
```

##Fit a miodel

```{r}
nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough)) +
  geom_point()
```

fit a model we care about 
```{r, include = FALSE}

fit <-  lm(price ~ stars + borough, data = nyc_airbnb)

summary(fit)
summary(fit)$coef
coef(fit)
fitted.values(fit)
residuals(fit)

```

lets look at the results better
```{r}
broom::glance(fit)

broom::tidy(fit) %>% 
  select(-std.error, -statistic) %>% 
  mutate(
    term = str_replace(term, "borough", "Borough: ")
  ) %>% 
  knitr::kable(dight = 3)


```


##Be in control of factors

```{r}
nyc_airbnb <- 
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type))
    
  
```
Look at the plot again 
```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)

broom::tidy(fit)
broom::glance(fit)
```

##Looking at diagnostocs

```{r}
modelr::add_residuals(nyc_airbnb, fit) %>% 
  ggplot(aes(x = borough, y = resid)) +
  geom_violin() +
  ylim(-500, 1500)


nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) +
  geom_point() +
  facet_wrap(.~ borough)
  
```
##HT

this does t test by defualt
```{r}
fit %>% 
  broom::tidy()

```
what about significance of borough as a whole ?
```{r}
fit_nul <- lm(price ~stars, data = nyc_airbnb)

fit_alt <-  lm(price ~ stars + borough, data = nyc_airbnb)

anova(fit_nul, fit_alt) %>% 
  broom::tidy()
```

## Nest data, fit models

```{r}
fit <- lm(price ~ stars*borough + room_type*borough, data = nyc_airbnb)

broom::tidy(fit)

```
More exploratory but easier to understand
```{r}
nyc_airbnb %>% 
  nest(data = -borough) %>% 
  mutate(
    models = map(.x = data, ~lm(price ~stars, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term == "stars")
```



##Model selection

```{r}
nonlin_df <- 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 -10*(x - .3)^2 + rnorm(100, 0, .3)
  )
```
Look at the data
```{r}
nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```
##Cross validation--by hand
```{r}
train_df <- sample_n(nonlin_df, size = 80)
test_df <- anti_join(nonlin_df, train_df, by = "id")
```
buiuld models of varying levels of complexity
we will fit three models
```{r}
linear_mod <- lm(y ~ x, data = train_df)
smooth_mod <- gam(y ~ s(x), data = train_df)
wiggly_mod <- gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)


```
reading what we just did

```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")

train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")

train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red") +
  facet_grid(.~model)


```
Looking at repdiction accuracy

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)


```
cross calidation using "omdelr"

```{r}
cv_df <- 
  crossv_mc(nonlin_df, 100)

cv_df %>% 
  pull(train) %>% 
  .[[1]] %>% 
as_tibble()

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

lets fit models an dget RMSE for them
```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(y ~ x, data = .x)),
    smooth_mod  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
    wiggly_mod  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y)))
```
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

##Boot strappong
Creating simulated data set
```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = sim_df_const %>% 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)
sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

sim_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 
```
One data set (constant) has met most assumptions(residual are costsant), the other one is not. 
 
we can solve this with bootstrap, it will try to create many other set of smapek staht look like thsi one, and use those to determine the actaul varainces of parameter estimates

```{r}
boot_sample = function(df){
  sample_frac(df, replace = TRUE) %>% 
    arrange(x)
}


boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ylim(-5, 16)

boot_sample(sim_df_nonconst) %>% 
  lm(y~x, data = .) %>% 
  broom::tidy()

```
##Now we do it with many sampels and analsysi
```{r}

boot_straps <- 
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
  )

boot_straps %>% pull(strap_sample) %>% .[[1]]
```
Analysis on this
```{r}

boot_results <- 
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm(y~x, data = .x),),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest()
```

What does the diratiution of the intercep of slope look like>
```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate), 
    sd_est = sd(estimate)
  )

```
Look at the distributions 
```{r}
boot_results %>% 
  filter(term == "x") %>% 
  ggplot(aes(x = estimate)) +
  geom_density()
  

```
Now we contruct a bootstrap confidence interval by chopping off some obseravtion from otyh end of the distribution

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  )

```

##Bootstarp using modelr
Can we simplify the process?yes!
```{r}
sim_df_nonconst %>% 
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(y~x, data = .x),),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate), 
    sd_est = sd(estimate)
  )
```

